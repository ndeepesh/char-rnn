{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Vanilla char-rnn implementation using Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = open('blog.txt').readlines()\n",
    "chars = list(''.join(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def oneHotEncodeChar(char_pos, input_size):\n",
    "    vec = np.zeros((1, input_size))\n",
    "    vec[0][char_pos] = 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def calculateLoss(inputs, targets, prev_hidden_state, Wih, Whh, Who, bh, bo):\n",
    "    # Make Forward and Backward Pass\n",
    "    loss = 0\n",
    "    hidden_state, output_vec, softmax_vec = {}, {}, {} # Save for each time-step to be used in back-prop step\n",
    "    hidden_state[-1] = prev_hidden_state\n",
    "        \n",
    "    # Forward Pass for all inputs collectively\n",
    "    for t in range(len(inputs)):\n",
    "        hidden_state[t] = np.tanh(np.dot(inputs[t], Wih) + np.dot(hidden_state[t-1], Whh) + bh) # 1xhidden_size\n",
    "        output_vec[t] = np.dot(hidden_state[t], Who) + bo # Unnormalized probabilities(1xoutput_size)\n",
    "        softmax_vec[t] = np.exp(output_vec[t]) / np.sum(np.exp(output_vec[t])) # Softmaxed Probablitites(1xoutput_size)\n",
    "        loss += -np.log(softmax_vec[t][0][np.where(targets[t] == 1)[0][0]])# Cross Entropy Loss(-y(t)*log(g(t)))\n",
    "            \n",
    "    # We first initialize all first order differentials of Weights and Biases w.r.t loss as zeros\n",
    "    dWih = np.zeros_like(Wih)\n",
    "    dWhh = np.zeros_like(Whh)\n",
    "    dWho = np.zeros_like(Who)\n",
    "    dbh = np.zeros_like(bh)\n",
    "    dbo = np.zeros_like(bo)\n",
    "    dhidden_state = np.zeros_like(prev_hidden_state)\n",
    "    dhidden_state_prev = np.zeros_like(prev_hidden_state)\n",
    "    \n",
    "    input_shape = inputs[0].shape\n",
    "    hidden_shape = prev_hidden_state.shape\n",
    "    output_shape = targets[0].shape\n",
    "    \n",
    "    # Backward-Pass for all inputs in reversed order\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        doutput_vec = softmax_vec[t] # 1xoutput_size\n",
    "        doutput_vec[np.where(targets[t] == 1)[0][0]] -= 1 # TODO: Show Derivation\n",
    "        assert(doutput_vec.shape == output_shape)\n",
    "        \n",
    "        assert(hidden_state[t].shape == hidden_shape)\n",
    "        dWho += np.dot(hidden_state[t].T, doutput_vec) #hidden_sizexoutput_size\n",
    "        dbo += doutput_vec# 1xoutput_size\n",
    "        assert(dWho.shape == Who.shape)\n",
    "        assert(dbo.shape == bo.shape)\n",
    "        \n",
    "        # So the hidden state for last char was only used to calculate output and not used within, i.e. we initialize dhidden_state_prev as 0\n",
    "        dhidden_state = np.dot(doutput_vec[np.where(targets[t] == 1)[0][0]].T, Who.T) + dhidden_state_prev # Since hidden state flows through output and within itself \n",
    "        assert(dhidden_state.shape == hidden_shape)\n",
    "        \n",
    "        dtanh = (1-hidden_state[t]**2)*dhidden_state\n",
    "        assert(dtanh.shape == hidden_shape)\n",
    "        \n",
    "        dWhh += np.dot(hidden_state[t-1].T, dtanh)\n",
    "        dbh += dtanh\n",
    "        dWih += np.dot(inputs[t].T, dtanh)\n",
    "        assert(dWhh.shape == Whh.shape)\n",
    "        assert(dbh.shape == bh.shape)\n",
    "        assert(dWih.shape == Wih.shape)\n",
    "        \n",
    "        dhidden_state_prev = np.dot(dtanh, Whh) # So if this backward pass is for last char, this dhidden_state_prev is for second last char\n",
    "        \n",
    "    # Taking care of exploding gradients. Making sure that they are neither too big nor too small, \n",
    "    # since this might be a problem with tanh activation function\n",
    "    for dparam in [dWih, dWhh, dWho, dbh, dbo]:\n",
    "        np.clip(dparam, -3, 3, out=dparam)\n",
    "    return loss, hidden_state[len(inputs)-1], dWih, dWhh, dWho, dbh, dbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def charRNN(inputs, hidden_size=50, batch_chars=20):\n",
    "    # Both input and output are one-hot encoded vectors\n",
    "    input_size = len(inputs)\n",
    "    output_size = input_size\n",
    "    \n",
    "    # Create dict for char-to-index mapping\n",
    "    #char_to_index = {ch:i for i, ch in enumerate(inputs)}\n",
    "    #index_to_char = {i:ch for i, ch in enumerate(inputs)}\n",
    "    \n",
    "    # Initialize weights(from a normal distribution with mean = 0.01) and biases\n",
    "    Wih = np.random.randn(input_size, hidden_size)*0.01\n",
    "    Whh = np.random.randn(hidden_size, hidden_size)*0.01\n",
    "    Who = np.random.randn(hidden_size, output_size)*0.01\n",
    "    bh = np.zeros((1, hidden_size))\n",
    "    bo = np.zeros((1, output_size))\n",
    "    \n",
    "    # Iterate over input_chars(till second last char), 'batch_chars' at a time\n",
    "    position=0\n",
    "    iterations = 0\n",
    "    prev_hidden_state = np.zeros((1, hidden_size)) # Initialize hidden state to all zeros\n",
    "    \n",
    "    # Initialize all momentum terms for Gradient Descent\n",
    "    momWih = np.zeros_like(Wih)\n",
    "    momWhh = np.zeros_like(Whh)\n",
    "    momWho = np.zeros_like(Who)\n",
    "    mombh = np.zeros_like(bh)\n",
    "    mombo = np.zeros_like(bo)\n",
    "    \n",
    "    learn_rate = 0.01\n",
    "    mom_rate = 0.9\n",
    "    \n",
    "    while(position < input_size-1):\n",
    "        sub_list_end = position + batch_chars\n",
    "        if(sub_list_end >= input_size-1):\n",
    "            sub_list_end = input_size-2 # Till second last char\n",
    "\n",
    "        if(position == sub_list_end): break\n",
    "        \n",
    "        input_sub_list = range(position, sub_list_end)\n",
    "        targets = range(position+1, sub_list_end+1) # Next char in the sequence is our desired output\n",
    "        \n",
    "        inputs_hot_encoded = []\n",
    "        targets_hot_encoded = []\n",
    "        for char_pos in input_sub_list: inputs_hot_encoded.append(oneHotEncodeChar(char_pos, input_size))\n",
    "        for char_pos in targets: targets_hot_encoded.append(oneHotEncodeChar(char_pos, input_size))\n",
    "\n",
    "        loss, prev_hidden_state, dWih, dWhh, dWho, dbh, dbo = calculateLoss(inputs_hot_encoded, targets_hot_encoded, \n",
    "                                                                 prev_hidden_state, Wih, Whh, Who, bh, bo)\n",
    "        \n",
    "        print(\"Iteration#\" + str(iterations) + \", Loss=\" + str(loss))\n",
    "        \n",
    "        # Momentum based Gradient-Descent\n",
    "        combined = zip([Wih, Whh, Who, bh, bo], [dWih, dWhh, dWho, dbh, dbo], [momWih, momWhh, momWho, mombh, mombo])\n",
    "        for param, dparam, momparam in combined:\n",
    "            momparam = mom_rate*momparam + learn_rate*dparam\n",
    "            param -= momparam\n",
    "\n",
    "        position = sub_list_end\n",
    "        iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration#0, Loss=176.897075136\n",
      "Iteration#1, Loss=176.852691298\n",
      "Iteration#2, Loss=176.853433021\n",
      "Iteration#3, Loss=176.021581137\n",
      "Iteration#4, Loss=175.307190448\n",
      "Iteration#5, Loss=175.2153371\n",
      "Iteration#6, Loss=175.260281088\n",
      "Iteration#7, Loss=175.265903101\n",
      "Iteration#8, Loss=175.266191006\n",
      "Iteration#9, Loss=175.266201871\n",
      "Iteration#10, Loss=175.266200174\n",
      "Iteration#11, Loss=175.266199034\n",
      "Iteration#12, Loss=175.266198236\n",
      "Iteration#13, Loss=175.266197613\n",
      "Iteration#14, Loss=175.26619713\n",
      "Iteration#15, Loss=175.266196766\n",
      "Iteration#16, Loss=175.266196447\n",
      "Iteration#17, Loss=175.26619621\n",
      "Iteration#18, Loss=175.266196008\n",
      "Iteration#19, Loss=175.266195852\n",
      "Iteration#20, Loss=175.266195707\n",
      "Iteration#21, Loss=175.266195591\n",
      "Iteration#22, Loss=175.266195496\n",
      "Iteration#23, Loss=175.266195411\n",
      "Iteration#24, Loss=175.266195336\n",
      "Iteration#25, Loss=175.266195275\n",
      "Iteration#26, Loss=175.266195213\n",
      "Iteration#27, Loss=175.266195156\n",
      "Iteration#28, Loss=175.266195114\n",
      "Iteration#29, Loss=175.266195073\n",
      "Iteration#30, Loss=175.266195038\n",
      "Iteration#31, Loss=175.266195008\n",
      "Iteration#32, Loss=175.266194977\n",
      "Iteration#33, Loss=175.266194951\n",
      "Iteration#34, Loss=175.266194929\n",
      "Iteration#35, Loss=175.266194907\n",
      "Iteration#36, Loss=175.266194885\n",
      "Iteration#37, Loss=175.26619487\n",
      "Iteration#38, Loss=175.266194852\n",
      "Iteration#39, Loss=175.266194837\n",
      "Iteration#40, Loss=175.266194825\n",
      "Iteration#41, Loss=175.266194808\n",
      "Iteration#42, Loss=175.266194797\n",
      "Iteration#43, Loss=175.266194787\n",
      "Iteration#44, Loss=175.266194778\n",
      "Iteration#45, Loss=175.266194765\n",
      "Iteration#46, Loss=175.266194758\n",
      "Iteration#47, Loss=175.266194749\n",
      "Iteration#48, Loss=175.266194741\n",
      "Iteration#49, Loss=175.266194734\n",
      "Iteration#50, Loss=175.266194727\n",
      "Iteration#51, Loss=175.266194719\n",
      "Iteration#52, Loss=175.266194715\n",
      "Iteration#53, Loss=175.266194707\n",
      "Iteration#54, Loss=175.266194703\n",
      "Iteration#55, Loss=175.266194697\n",
      "Iteration#56, Loss=175.266194692\n",
      "Iteration#57, Loss=175.266194688\n",
      "Iteration#58, Loss=175.266194683\n",
      "Iteration#59, Loss=175.266194679\n",
      "Iteration#60, Loss=175.266194676\n",
      "Iteration#61, Loss=175.266194673\n",
      "Iteration#62, Loss=175.266194669\n",
      "Iteration#63, Loss=175.266194666\n",
      "Iteration#64, Loss=175.266194662\n",
      "Iteration#65, Loss=175.266194659\n",
      "Iteration#66, Loss=175.266194656\n",
      "Iteration#67, Loss=175.266194653\n",
      "Iteration#68, Loss=175.26619465\n",
      "Iteration#69, Loss=175.266194648\n",
      "Iteration#70, Loss=175.266194645\n",
      "Iteration#71, Loss=175.266194643\n",
      "Iteration#72, Loss=175.26619464\n",
      "Iteration#73, Loss=175.266194639\n",
      "Iteration#74, Loss=175.266194637\n",
      "Iteration#75, Loss=175.266194634\n",
      "Iteration#76, Loss=175.266194633\n",
      "Iteration#77, Loss=175.266194632\n",
      "Iteration#78, Loss=175.266194629\n",
      "Iteration#79, Loss=175.266194627\n",
      "Iteration#80, Loss=175.266194626\n",
      "Iteration#81, Loss=175.266194625\n",
      "Iteration#82, Loss=175.266194623\n",
      "Iteration#83, Loss=175.266194622\n",
      "Iteration#84, Loss=175.26619462\n",
      "Iteration#85, Loss=175.266194619\n",
      "Iteration#86, Loss=175.266194618\n",
      "Iteration#87, Loss=175.266194616\n",
      "Iteration#88, Loss=175.266194616\n",
      "Iteration#89, Loss=175.266194614\n",
      "Iteration#90, Loss=175.266194613\n",
      "Iteration#91, Loss=175.266194612\n",
      "Iteration#92, Loss=175.26619461\n",
      "Iteration#93, Loss=175.26619461\n",
      "Iteration#94, Loss=175.266194609\n",
      "Iteration#95, Loss=175.266194608\n",
      "Iteration#96, Loss=175.266194607\n",
      "Iteration#97, Loss=175.266194606\n",
      "Iteration#98, Loss=175.266194605\n",
      "Iteration#99, Loss=175.266194605\n",
      "Iteration#100, Loss=175.266194604\n",
      "Iteration#101, Loss=175.266194603\n",
      "Iteration#102, Loss=175.266194602\n",
      "Iteration#103, Loss=175.266194601\n",
      "Iteration#104, Loss=175.266194601\n",
      "Iteration#105, Loss=175.2661946\n",
      "Iteration#106, Loss=175.266194599\n",
      "Iteration#107, Loss=175.266194598\n",
      "Iteration#108, Loss=175.266194598\n",
      "Iteration#109, Loss=175.266194597\n",
      "Iteration#110, Loss=175.266194597\n",
      "Iteration#111, Loss=175.266194596\n",
      "Iteration#112, Loss=175.266194596\n",
      "Iteration#113, Loss=175.266194595\n",
      "Iteration#114, Loss=175.266194594\n",
      "Iteration#115, Loss=175.266194594\n",
      "Iteration#116, Loss=175.266194593\n",
      "Iteration#117, Loss=175.266194593\n",
      "Iteration#118, Loss=175.266194592\n",
      "Iteration#119, Loss=175.266194592\n",
      "Iteration#120, Loss=175.266194592\n",
      "Iteration#121, Loss=175.266194591\n",
      "Iteration#122, Loss=175.26619459\n",
      "Iteration#123, Loss=175.26619459\n",
      "Iteration#124, Loss=175.26619459\n",
      "Iteration#125, Loss=175.266194589\n",
      "Iteration#126, Loss=175.266194589\n",
      "Iteration#127, Loss=175.266194589\n",
      "Iteration#128, Loss=175.266194588\n",
      "Iteration#129, Loss=175.266194588\n",
      "Iteration#130, Loss=175.266194587\n",
      "Iteration#131, Loss=175.266194587\n",
      "Iteration#132, Loss=175.266194587\n",
      "Iteration#133, Loss=175.266194587\n",
      "Iteration#134, Loss=175.266194586\n",
      "Iteration#135, Loss=175.266194586\n",
      "Iteration#136, Loss=175.266194586\n",
      "Iteration#137, Loss=175.266194585\n",
      "Iteration#138, Loss=175.266194585\n",
      "Iteration#139, Loss=175.266194585\n",
      "Iteration#140, Loss=175.266194584\n",
      "Iteration#141, Loss=175.266194584\n",
      "Iteration#142, Loss=175.266194584\n",
      "Iteration#143, Loss=175.266194584\n",
      "Iteration#144, Loss=175.266194583\n",
      "Iteration#145, Loss=175.266194583\n",
      "Iteration#146, Loss=175.266194583\n",
      "Iteration#147, Loss=175.266194582\n",
      "Iteration#148, Loss=175.266194582\n",
      "Iteration#149, Loss=175.266194582\n",
      "Iteration#150, Loss=175.266194582\n",
      "Iteration#151, Loss=175.266194582\n",
      "Iteration#152, Loss=175.266194581\n",
      "Iteration#153, Loss=175.266194581\n",
      "Iteration#154, Loss=175.266194581\n",
      "Iteration#155, Loss=175.266194581\n",
      "Iteration#156, Loss=175.26619458\n",
      "Iteration#157, Loss=175.26619458\n",
      "Iteration#158, Loss=175.26619458\n",
      "Iteration#159, Loss=175.26619458\n",
      "Iteration#160, Loss=175.26619458\n",
      "Iteration#161, Loss=175.266194579\n",
      "Iteration#162, Loss=175.266194579\n",
      "Iteration#163, Loss=175.266194579\n",
      "Iteration#164, Loss=175.266194579\n",
      "Iteration#165, Loss=175.266194579\n",
      "Iteration#166, Loss=175.266194578\n",
      "Iteration#167, Loss=175.266194578\n",
      "Iteration#168, Loss=175.266194578\n",
      "Iteration#169, Loss=175.266194578\n",
      "Iteration#170, Loss=175.266194578\n",
      "Iteration#171, Loss=175.266194578\n",
      "Iteration#172, Loss=175.266194578\n",
      "Iteration#173, Loss=175.266194578\n",
      "Iteration#174, Loss=175.266194577\n",
      "Iteration#175, Loss=175.266194577\n",
      "Iteration#176, Loss=175.266194577\n",
      "Iteration#177, Loss=175.266194577\n",
      "Iteration#178, Loss=175.266194577\n",
      "Iteration#179, Loss=175.266194577\n",
      "Iteration#180, Loss=175.266194576\n",
      "Iteration#181, Loss=175.266194576\n",
      "Iteration#182, Loss=175.266194576\n",
      "Iteration#183, Loss=175.266194576\n",
      "Iteration#184, Loss=175.266194576\n",
      "Iteration#185, Loss=175.266194576\n",
      "Iteration#186, Loss=175.266194576\n",
      "Iteration#187, Loss=175.266194576\n",
      "Iteration#188, Loss=175.266194575\n",
      "Iteration#189, Loss=175.266194575\n",
      "Iteration#190, Loss=175.266194575\n",
      "Iteration#191, Loss=175.266194575\n",
      "Iteration#192, Loss=175.266194575\n",
      "Iteration#193, Loss=175.266194575\n",
      "Iteration#194, Loss=175.266194575\n",
      "Iteration#195, Loss=175.266194575\n",
      "Iteration#196, Loss=175.266194575\n",
      "Iteration#197, Loss=175.266194574\n",
      "Iteration#198, Loss=175.266194574\n",
      "Iteration#199, Loss=175.266194574\n",
      "Iteration#200, Loss=175.266194574\n",
      "Iteration#201, Loss=175.266194574\n",
      "Iteration#202, Loss=175.266194574\n",
      "Iteration#203, Loss=175.266194574\n",
      "Iteration#204, Loss=175.266194574\n",
      "Iteration#205, Loss=175.266194574\n",
      "Iteration#206, Loss=175.266194574\n",
      "Iteration#207, Loss=175.266194574\n",
      "Iteration#208, Loss=175.266194574\n",
      "Iteration#209, Loss=175.266194573\n",
      "Iteration#210, Loss=175.266194573\n",
      "Iteration#211, Loss=175.266194573\n",
      "Iteration#212, Loss=175.266194573\n",
      "Iteration#213, Loss=175.266194573\n",
      "Iteration#214, Loss=175.266194573\n",
      "Iteration#215, Loss=175.266194573\n",
      "Iteration#216, Loss=175.266194573\n",
      "Iteration#217, Loss=175.266194573\n",
      "Iteration#218, Loss=175.266194573\n",
      "Iteration#219, Loss=175.266194573\n",
      "Iteration#220, Loss=175.266194573\n",
      "Iteration#221, Loss=175.266194572\n",
      "Iteration#222, Loss=175.266194572\n",
      "Iteration#223, Loss=175.266194572\n",
      "Iteration#224, Loss=175.266194572\n",
      "Iteration#225, Loss=175.266194572\n",
      "Iteration#226, Loss=175.266194572\n",
      "Iteration#227, Loss=175.266194572\n",
      "Iteration#228, Loss=175.266194572\n",
      "Iteration#229, Loss=175.266194572\n",
      "Iteration#230, Loss=175.266194572\n",
      "Iteration#231, Loss=175.266194572\n",
      "Iteration#232, Loss=175.266194572\n",
      "Iteration#233, Loss=175.266194572\n",
      "Iteration#234, Loss=175.266194572\n",
      "Iteration#235, Loss=175.266194572\n",
      "Iteration#236, Loss=175.266194572\n",
      "Iteration#237, Loss=175.266194571\n",
      "Iteration#238, Loss=175.266194571\n",
      "Iteration#239, Loss=175.266194571\n",
      "Iteration#240, Loss=175.266194571\n",
      "Iteration#241, Loss=175.266194571\n",
      "Iteration#242, Loss=175.266194571\n",
      "Iteration#243, Loss=175.266194571\n",
      "Iteration#244, Loss=175.266194571\n",
      "Iteration#245, Loss=175.266194571\n",
      "Iteration#246, Loss=175.266194571\n",
      "Iteration#247, Loss=175.266194571\n",
      "Iteration#248, Loss=175.266194571\n",
      "Iteration#249, Loss=175.266194571\n",
      "Iteration#250, Loss=175.266194571\n",
      "Iteration#251, Loss=175.266194571\n",
      "Iteration#252, Loss=175.266194571\n",
      "Iteration#253, Loss=175.266194571\n",
      "Iteration#254, Loss=175.266194571\n",
      "Iteration#255, Loss=175.266194571\n",
      "Iteration#256, Loss=175.26619457\n",
      "Iteration#257, Loss=175.26619457\n",
      "Iteration#258, Loss=175.26619457\n",
      "Iteration#259, Loss=175.26619457\n",
      "Iteration#260, Loss=175.26619457\n",
      "Iteration#261, Loss=175.26619457\n",
      "Iteration#262, Loss=175.26619457\n",
      "Iteration#263, Loss=175.26619457\n",
      "Iteration#264, Loss=175.26619457\n",
      "Iteration#265, Loss=175.26619457\n",
      "Iteration#266, Loss=175.26619457\n",
      "Iteration#267, Loss=175.26619457\n",
      "Iteration#268, Loss=175.26619457\n",
      "Iteration#269, Loss=175.26619457\n",
      "Iteration#270, Loss=175.26619457\n",
      "Iteration#271, Loss=175.26619457\n",
      "Iteration#272, Loss=175.26619457\n",
      "Iteration#273, Loss=175.26619457\n",
      "Iteration#274, Loss=175.26619457\n",
      "Iteration#275, Loss=175.26619457\n",
      "Iteration#276, Loss=175.26619457\n",
      "Iteration#277, Loss=175.26619457\n",
      "Iteration#278, Loss=175.26619457\n",
      "Iteration#279, Loss=175.26619457\n",
      "Iteration#280, Loss=175.26619457\n",
      "Iteration#281, Loss=175.26619457\n",
      "Iteration#282, Loss=175.266194569\n",
      "Iteration#283, Loss=175.266194569\n",
      "Iteration#284, Loss=175.266194569\n",
      "Iteration#285, Loss=175.266194569\n",
      "Iteration#286, Loss=175.266194569\n",
      "Iteration#287, Loss=175.266194569\n",
      "Iteration#288, Loss=175.266194569\n",
      "Iteration#289, Loss=175.266194569\n",
      "Iteration#290, Loss=175.266194569\n",
      "Iteration#291, Loss=175.266194569\n",
      "Iteration#292, Loss=175.266194569\n",
      "Iteration#293, Loss=175.266194569\n",
      "Iteration#294, Loss=175.266194569\n",
      "Iteration#295, Loss=175.266194569\n",
      "Iteration#296, Loss=175.266194569\n",
      "Iteration#297, Loss=175.266194569\n",
      "Iteration#298, Loss=175.266194569\n",
      "Iteration#299, Loss=175.266194569\n",
      "Iteration#300, Loss=175.266194569\n",
      "Iteration#301, Loss=175.266194569\n",
      "Iteration#302, Loss=175.266194569\n",
      "Iteration#303, Loss=175.266194569\n",
      "Iteration#304, Loss=175.266194569\n",
      "Iteration#305, Loss=175.266194569\n",
      "Iteration#306, Loss=175.266194569\n",
      "Iteration#307, Loss=175.266194569\n",
      "Iteration#308, Loss=175.266194569\n",
      "Iteration#309, Loss=175.266194569\n",
      "Iteration#310, Loss=175.266194569\n",
      "Iteration#311, Loss=175.266194569\n",
      "Iteration#312, Loss=175.266194569\n",
      "Iteration#313, Loss=175.266194569\n",
      "Iteration#314, Loss=175.266194569\n",
      "Iteration#315, Loss=175.266194568\n",
      "Iteration#316, Loss=175.266194568\n",
      "Iteration#317, Loss=175.266194568\n",
      "Iteration#318, Loss=175.266194568\n",
      "Iteration#319, Loss=175.266194568\n",
      "Iteration#320, Loss=175.266194568\n",
      "Iteration#321, Loss=175.266194568\n",
      "Iteration#322, Loss=175.266194568\n",
      "Iteration#323, Loss=175.266194568\n",
      "Iteration#324, Loss=175.266194568\n",
      "Iteration#325, Loss=175.266194568\n",
      "Iteration#326, Loss=175.266194568\n",
      "Iteration#327, Loss=175.266194568\n",
      "Iteration#328, Loss=175.266194568\n",
      "Iteration#329, Loss=175.266194568\n",
      "Iteration#330, Loss=175.266194568\n",
      "Iteration#331, Loss=175.266194568\n",
      "Iteration#332, Loss=175.266194568\n",
      "Iteration#333, Loss=175.266194568\n",
      "Iteration#334, Loss=175.266194568\n",
      "Iteration#335, Loss=175.266194568\n",
      "Iteration#336, Loss=175.266194568\n",
      "Iteration#337, Loss=175.266194568\n",
      "Iteration#338, Loss=175.266194568\n",
      "Iteration#339, Loss=175.266194568\n",
      "Iteration#340, Loss=175.266194568\n",
      "Iteration#341, Loss=175.266194568\n",
      "Iteration#342, Loss=175.266194568\n",
      "Iteration#343, Loss=175.266194568\n",
      "Iteration#344, Loss=175.266194568\n",
      "Iteration#345, Loss=175.266194568\n",
      "Iteration#346, Loss=148.976265383\n"
     ]
    }
   ],
   "source": [
    "charRNN(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
